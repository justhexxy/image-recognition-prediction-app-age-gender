# -*- coding: utf-8 -*-
"""no-gpt-generated-column-prompt-engineering-assignment4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CrLI74MqWh2f8PKEvDZywom41OSvLr21
"""

# Assignment: Performance Evaluation Before and After Fine-Tuning with LoRA

## Introduction
# In this notebook, we will evaluate the performance of a pre-trained language model (GPT-2) on the task of generating headlines from full news articles.
# We will first evaluate the model's performance before fine-tuning, then fine-tune the model using LoRA, and finally, evaluate the performance after fine-tuning to measure any improvements.
# Import necessary libraries
# Install the necessary libraries
!pip install datasets transformers rouge_score
#!pip install nvidia-cublas-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvrtc-cu12 nvidia-cuda-runtime-cu12 nvidia-cudnn-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-nccl-cu12 nvidia-nvtx-cu12

# Import necessary libraries
from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import Dataset, load_metric
import pandas as pd

import torch
torch.cuda.is_available()

# Load datasets
train_df = pd.read_csv('train_headline_dataset.csv')
val_df = pd.read_csv('val_headline_dataset.csv')
test_df = pd.read_csv('test_headline_dataset.csv')

# Display the datasets
print("Training Data Sample:")
display(train_df.head())

print("Validation Data Sample:")
display(val_df.head())

print("Test Data Sample:")
display(test_df.head())

# Initialize the tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2-xl")
tokenizer.pad_token = tokenizer.eos_token

# Function to tokenize the data
def tokenize_function(examples):
    return tokenizer(examples['full_article'], padding='max_length', truncation=True)

# Apply the tokenization
train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)
test_dataset = Dataset.from_pandas(test_df)

# Store original titles separately for evaluation
original_val_titles = val_df['original_title'].tolist()
original_test_titles = test_df['original_title'].tolist()

train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# Remove columns not required by the model
train_dataset = train_dataset.remove_columns(['full_article', 'original_title'])
val_dataset = val_dataset.remove_columns(['full_article', 'original_title'])
test_dataset = test_dataset.remove_columns(['full_article', 'original_title'])

# Set the format of the dataset to PyTorch tensors
train_dataset.set_format('torch')
val_dataset.set_format('torch')
test_dataset.set_format('torch')

# DataCollator for Language Modeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Load BLEU and ROUGE metrics
bleu = load_metric("bleu",trust_remote_code=True)
rouge = load_metric("rouge",trust_remote_code=True)

# Function to evaluate the model
def evaluate_model(model, tokenizer, dataset, original_titles):
    generated_titles = []
    for i, item in enumerate(dataset):
        inputs = {key: item[key].unsqueeze(0) for key in item.keys()}
        outputs = model.generate(**inputs, max_new_tokens=20, num_return_sequences=1)
        generated_title = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_titles.append(generated_title)

    return generated_titles, original_titles

# Load pre-trained GPT-2 model and evaluate on validation set
model = GPT2LMHeadModel.from_pretrained("gpt2-xl")
generated_titles, original_titles = evaluate_model(model, tokenizer, val_dataset, original_val_titles)

# Display generated titles before fine-tuning
for i in range(len(generated_titles)):
    print(f"Original Title: {original_titles[i]}")
    print(f"Generated Title: {generated_titles[i]}")
    print("")

# Prepare BLEU and ROUGE inputs
references = [[title.split()] for title in original_titles]
predictions = [title.split() for title in generated_titles]

# Calculate BLEU and ROUGE scores before fine-tuning
bleu_score = bleu.compute(predictions=predictions, references=references)
rouge_score = rouge.compute(predictions=generated_titles, references=original_titles)

print("BLEU Score Before Fine-Tuning:", bleu_score)
print("ROUGE Score Before Fine-Tuning:", rouge_score)

# Define training arguments for the Trainer
training_args = TrainingArguments(
    output_dir="./results",           # Directory where the model's checkpoints and outputs will be saved
    overwrite_output_dir=True,        # Overwrite the output directory if it already exists
    num_train_epochs=3,               # Number of training epochs (passes through the entire dataset)
    per_device_train_batch_size=4,    # Batch size per device during training (e.g., if you have 2 GPUs, the effective batch size will be 8)
    save_steps=10_000,                # Save a checkpoint of the model every 10,000 steps
    save_total_limit=2,               # Limit the total number of checkpoints to 2, older checkpoints will be deleted
    remove_unused_columns=False       # Retain all columns in the dataset, even those not used by the model (useful when you want to keep data for logging or custom processing)
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=data_collator,
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./fine_tuned_model")
tokenizer.save_pretrained("./fine_tuned_model")

# Evaluate the fine-tuned model on the test set
fine_tuned_model = GPT2LMHeadModel.from_pretrained("./fine_tuned_model")
generated_titles_after_ft, original_titles_after_ft = evaluate_model(fine_tuned_model, tokenizer, test_dataset, original_test_titles)

# Display generated titles after fine-tuning
for i in range(len(generated_titles_after_ft)):
    print(f"Original Title: {original_titles_after_ft[i]}")
    print(f"Generated Title: {generated_titles_after_ft[i]}")
    print("")

# Prepare BLEU and ROUGE inputs after fine-tuning
references_ft = [[title.split()] for title in original_titles_after_ft]
predictions_ft = [title.split() for title in generated_titles_after_ft]

bleu_score_ft = bleu.compute(predictions=predictions_ft, references=references_ft)
rouge_score_ft = rouge.compute(predictions=generated_titles_after_ft, references=original_titles_after_ft)

print("BLEU Score After Fine-Tuning:", bleu_score_ft)
print("ROUGE Score After Fine-Tuning:", rouge_score_ft)

# Compare the scores before and after fine-tuning
print("\n--- Score Comparison ---")
print("BLEU Score Before Fine-Tuning:", bleu_score)
print("BLEU Score After Fine-Tuning:", bleu_score_ft)

print("ROUGE Score Before Fine-Tuning:", rouge_score)
print("ROUGE Score After Fine-Tuning:", rouge_score_ft)