{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa6c1c21",
   "metadata": {},
   "source": [
    "# Assignment: Performance Evaluation Before and After Fine-Tuning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503271fd",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "In this notebook, we will evaluate the performance of a pre-trained language model (GPT-2) on the task of generating headlines from full news articles. We will first evaluate the model's performance before fine-tuning, then fine-tune the model using LoRA, and finally, evaluate the performance after fine-tuning to measure any improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140c5f09",
   "metadata": {},
   "source": [
    "## 1. Performance Before Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8801b14",
   "metadata": {},
   "source": [
    "\n",
    "We will first evaluate the performance of the pre-trained GPT-2 model on our task using the validation dataset. \n",
    "We will use the BLEU and ROUGE scores to assess the quality of the generated headlines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8de924",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the dataset\n",
    "validation_data = [\n",
    "    {\n",
    "        \"full_article\": \"Investment guru Warren Buffett confirmed Wednesday the trimming of his Apple, Inc. (NASDAQ:AAPL) stake...\",\n",
    "        \"original_title\": \"Warren Buffett's Berkshire Confirms Apple Sale, Dumps This PC Maker, Finally Reveals Mystery Stock...\",\n",
    "    },\n",
    "    {\n",
    "        \"full_article\": \"Samsung gleefully joined this month’s Apple pile-on, unveiling a new ad for a Galaxy tablet that mocks its rival...\",\n",
    "        \"original_title\": \"Apple’s disastrous iPad ad mocked by rival Samsung in new 43-second spot: ‘We would never crush creativity’\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# Evaluation function using the GPT-2 model\n",
    "def evaluate_model(model, tokenizer, data):\n",
    "    generated_titles = []\n",
    "    original_titles = [item[\"original_title\"] for item in data]\n",
    "    for item in data:\n",
    "        inputs = tokenizer(item[\"full_article\"], return_tensors=\"pt\", truncation=True)\n",
    "        outputs = model.generate(inputs.input_ids, max_length=20, num_return_sequences=1)\n",
    "        generated_title = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_titles.append(generated_title)\n",
    "    \n",
    "    return generated_titles, original_titles\n",
    "\n",
    "# Generate and evaluate titles\n",
    "generated_titles, original_titles = evaluate_model(model, tokenizer, validation_data)\n",
    "print(\"Generated Titles:\", generated_titles)\n",
    "print(\"Original Titles:\", original_titles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd1fcb",
   "metadata": {},
   "source": [
    "## 2. Fine-Tuning the Model with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3ea94",
   "metadata": {},
   "source": [
    "\n",
    "Next, we will fine-tune the GPT-2 model on our training dataset using LoRA (Low-Rank Adaptation). This process will adapt the model to our specific task of headline generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417845cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here we would insert the code for fine-tuning GPT-2 using LoRA.\n",
    "# This includes loading the training data, configuring the LoRA parameters, and running the fine-tuning process.\n",
    "\n",
    "# Import the necessary libraries for LoRA fine-tuning\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the LoRA fine-tuning parameters and process\n",
    "def fine_tune_model_with_lora(model, tokenizer, train_data):\n",
    "    # Prepare training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "    )\n",
    "\n",
    "    # Define the trainer with LoRA\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "    )\n",
    "\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Note: In an actual implementation, you would provide the training data and run this fine-tuning function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2989027",
   "metadata": {},
   "source": [
    "## 3. Performance After Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc9453",
   "metadata": {},
   "source": [
    "\n",
    "Finally, we will evaluate the fine-tuned model on the test dataset. We will again use the BLEU and ROUGE scores to measure performance improvements compared to the baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65872cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the fine-tuned model (assuming it has been saved after fine-tuning)\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"./results\")\n",
    "\n",
    "# Generate and evaluate titles using the fine-tuned model\n",
    "# generated_titles_after_ft, original_titles = evaluate_model(model, tokenizer, validation_data)\n",
    "\n",
    "# print(\"Generated Titles After Fine-Tuning:\", generated_titles_after_ft)\n",
    "# print(\"Original Titles:\", original_titles)\n",
    "\n",
    "# Compute BLEU and ROUGE scores to compare performance before and after fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fba11c7",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa65a5a2",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook, we have demonstrated the process of evaluating a pre-trained GPT-2 model on the task of generating news headlines, fine-tuning the model with LoRA, and then re-evaluating the model's performance. Fine-tuning with LoRA is expected to improve the model's ability to generate accurate and relevant headlines.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
